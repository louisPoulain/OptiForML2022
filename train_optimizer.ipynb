{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train optimizer\n",
    "Notebook to train different optimizer to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from optimizer.AdaHessian import AdaHessian\n",
    "from optimizer.Atmo import Atmo, MASScheduler\n",
    "from dataset import ImagesDataset\n",
    "from model import ResNet18\n",
    "from path import TRAIN_HISTORY_DIR, TRAIN_MODEL_DIR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose dataset\n",
    "data_name = \"MNIST\"\n",
    "#data_name = \"cifar\"\n",
    "full = False\n",
    "tiny = True\n",
    "\n",
    "# choose optimizer\n",
    "optimizer_name = \"adam\"\n",
    "optimizer_name = \"sgd\"\n",
    "optimizer_name = \"atmo\"\n",
    "optimizer_name = \"dynamicAtom\"\n",
    "#optimizer_name = \"adaHessian\"\n",
    "\n",
    "# choose nb of epochs\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set to the tiny setup\n",
      "** Use 500 train samples\n",
      "* Using MNIST\n",
      "** Reduce the data-set to the tiny setup\n",
      "** Use 100 test samples\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "train_dataset = ImagesDataset(full = full, tiny=tiny, cifar=(data_name==\"cifar\"))\n",
    "test_dataset = ImagesDataset(full = full, tiny=tiny, cifar=(data_name==\"cifar\"), test=True)\n",
    "\n",
    "\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = ResNet18(in_channel=1 if data_name==\"MNIST\" else 3)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "# load optimizer\n",
    "if optimizer_name == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "if optimizer_name == \"sgd\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "if optimizer_name == \"atmo\":\n",
    "    optimizer = Atmo(model.parameters())\n",
    "if optimizer_name == \"dynamicAtom\":\n",
    "    optimizer = Atmo(model.parameters(), adam_w=1, sgd_w=0)\n",
    "    dynamic = MASScheduler(optimizer, epochs = epochs)\n",
    "if optimizer_name == \"adaHessian\":\n",
    "    optimizer = AdaHessian(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7141dc8672dc4e7481191d3a97f15256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa947a0e58e74f46b87a0b07fda27132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 1:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b36fd7e935f4155b2c279f19ef30b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test epoch 1:   0%|          | 0/1 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06aaf121e9846dc8152d6c1470dc259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 2:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3fd9400d954f6abf6b944344ddee91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test epoch 2:   0%|          | 0/1 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "time_epoch = []\n",
    "\n",
    "with trange(1, epochs + 1, desc='Training', unit='epoch') as t:\n",
    "  for epoch in t:\n",
    "    losses = []\n",
    "    acc = []\n",
    "    start_time = time.time()\n",
    "    with tqdm(trainDataLoader, desc=f'Train epoch {epoch}',\n",
    "              unit='batch', leave=False) as t1:\n",
    "      for x_train, y_train in t1:\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward(create_graph=(optimizer_name==\"adaHessian\"))\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss)\n",
    "        pred = torch.argmax(output, axis = 1)\n",
    "        acc.append(sum(pred == y_train).item()/pred.shape[0])\n",
    "\n",
    "    if optimizer_name==\"dynamicAtmo\":\n",
    "          dynamic.step()\n",
    "\n",
    "    train_loss.append(sum(losses)/len(losses))\n",
    "    train_acc.append(sum(acc)/len(acc))\n",
    "\n",
    "    losses = []\n",
    "    acc = []\n",
    "    with torch.no_grad():\n",
    "      with tqdm(testDataLoader, desc=f'Test epoch {epoch}',\n",
    "                unit='batch', leave=False) as t1:\n",
    "        for x_test, y_test in t1:\n",
    "          x_test = x_test.to(device)\n",
    "          y_test = y_test.to(device)\n",
    "\n",
    "          output = model(x_test)\n",
    "          loss = criterion(output, y_test)\n",
    "          losses.append(loss)\n",
    "\n",
    "          pred = torch.argmax(output, axis = 1)\n",
    "          acc.append(sum(pred == y_test).item()/pred.shape[0])\n",
    "\n",
    "      test_loss.append(sum(losses)/len(losses))\n",
    "      test_acc.append(sum(acc)/len(acc))\n",
    "\n",
    "      end_time = time.time()\n",
    "      time_epoch.append(end_time-start_time)\n",
    "\n",
    "#mean by epoch\n",
    "time_epoch = sum(time_epoch)/len(time_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'd:\\\\Emilien\\\\Documents\\\\Cours\\\\MA2\\\\Optimization for ML\\\\project\\\\output\\\\training_history\\\\dynamicAtom_2epochs_MNIST.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1124/2409659335.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"optimizer\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_HISTORY_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{optimizer_name}_{epochs}epochs_{data_name}.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m   \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'd:\\\\Emilien\\\\Documents\\\\Cours\\\\MA2\\\\Optimization for ML\\\\project\\\\output\\\\training_history\\\\dynamicAtom_2epochs_MNIST.pickle'"
     ]
    }
   ],
   "source": [
    "# run this cell to save history\n",
    "history = dict()\n",
    "history[\"train_loss\"] = train_loss\n",
    "history[\"train_acc\"] = train_acc\n",
    "history[\"test_loss\"] = test_loss\n",
    "history[\"test_acc\"] = test_acc\n",
    "history[\"time_epoch\"] = time_epoch\n",
    "history[\"data\"] = data_name\n",
    "history[\"model\"] = \"resnet18\"\n",
    "history[\"epochs\"] = epochs\n",
    "history[\"optimizer\"] = optimizer_name\n",
    "with open(os.path.join(TRAIN_HISTORY_DIR, f\"{optimizer_name}_{epochs}epochs_{data_name}.pickle\"), 'wb') as f:\n",
    "  pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'd:\\\\Emilien\\\\Documents\\\\Cours\\\\MA2\\\\Optimization for ML\\\\project\\\\output\\\\pretrained_model\\\\dynamicAtom_2epochs_MNIST.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1124/2938124463.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# run this cell to save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_MODEL_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{optimizer_name}_{epochs}epochs_{data_name}.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'd:\\\\Emilien\\\\Documents\\\\Cours\\\\MA2\\\\Optimization for ML\\\\project\\\\output\\\\pretrained_model\\\\dynamicAtom_2epochs_MNIST.pt'"
     ]
    }
   ],
   "source": [
    "# run this cell to save model\n",
    "weights = model.state_dict()\n",
    "torch.save(weights,os.path.join(TRAIN_MODEL_DIR, f\"{optimizer_name}_{epochs}epochs_{data_name}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "100425e406423ce956545b95bc1f6e6855675069bc773847139c573c06d47449"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('jupyterlab-debugger')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
